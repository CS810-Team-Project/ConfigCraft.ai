{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "# llm = LlamaCPP(\n",
    "#     # You can pass in the URL to a GGML model to download it automatically\n",
    "#     model_url=model_url,\n",
    "#     # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "#     model_path=None,\n",
    "#     temperature=0.1,\n",
    "#     max_new_tokens=256,\n",
    "#     # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "#     context_window=3900,\n",
    "#     # kwargs to pass to __call__()\n",
    "#     generate_kwargs={},\n",
    "#     # kwargs to pass to __init__()\n",
    "#     # set to at least 1 to use GPU\n",
    "#     model_kwargs={\"n_gpu_layers\": 5},\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"codellama\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"codellama\", \n",
    "    request_timeout=120.0,\n",
    "    system_prompt=\"You are a senior developer specializing in static analysis tools and secure operating systems. You will follow the input instructions and generate code. Your output will be purely code, without additional information.\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"123456\"\n",
    "port = \"5432\"\n",
    "user = \"evan\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(\"\"\"\n",
    "        SELECT pg_terminate_backend(pg_stat_activity.pid)\n",
    "        FROM pg_stat_activity\n",
    "        WHERE pg_stat_activity.datname = 'vector_db'\n",
    "        AND pid <> pg_backend_pid();\n",
    "    \"\"\")\n",
    "    c.execute(\"DROP DATABASE IF EXISTS vector_db;\")\n",
    "conn.close()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(\"CREATE DATABASE vector_db;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"codellama\",\n",
    "    embed_dim=4096,  # llama embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Ingestion Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29 docs\n"
     ]
    }
   ],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/sootsurvivorsguide.pdf\")\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "required_exts = [\".md\", \".java\"]\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"/home/ubuntu/ConfigCraft.ai/experimental/data/soot_doc\",\n",
    "    required_exts=required_exts,\n",
    "    recursive=True,\n",
    ")\n",
    "docs = reader.load_data()\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "# combine the two document lists\n",
    "documents = documents + docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Text Splitter to Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Construct Nodes from Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87 nodes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings for each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def get_and_set_embedding(node):\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding\n",
    "    return node\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    nodes = list(executor.map(get_and_set_embedding, nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Nodes into a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15fff635-bc96-4f7e-974d-54f5f1e6c09b',\n",
       " '99afa511-36da-46ef-abbf-381486449723',\n",
       " '5656bc28-f806-4ce4-a4bc-c45f41ce690e',\n",
       " 'a937b3f0-e105-426a-9c16-49019cc03811',\n",
       " 'a2ced6c5-0524-46d6-a414-ec6e1ba6188d',\n",
       " '48251e1c-9320-410b-87b5-ce2c58801b8d',\n",
       " '18bf7c64-17ad-4ffe-b03a-c83542fe5120',\n",
       " '0f42686b-f8d7-4e63-b40b-03f3d1c469e6',\n",
       " '309ef8d1-cf03-46a5-8093-7727437adf29',\n",
       " 'e0a4a277-d445-4c9c-bfc4-200df30369e0',\n",
       " '630f8b94-7ccd-49f6-bd1a-4f7cf74cf40f',\n",
       " '372c2dd0-d9f3-4685-9203-4af701a18ffe',\n",
       " '0748889e-8f11-40e0-a313-caf045aba30a',\n",
       " '43f61347-08a4-4f72-bf80-eb6d1f404469',\n",
       " 'f20f7694-a72c-47a1-8c90-3aba4982d427',\n",
       " '13f7cdb9-0d93-4745-befb-d23f692e14aa',\n",
       " '13109f01-e3bb-46ae-a725-1cfee9966326',\n",
       " '2f223b47-7fce-49a0-9477-e1a15a71ff3d',\n",
       " '1a0706d2-0761-4193-bc7b-6f5172b4b83b',\n",
       " '61de95ef-7582-4086-9555-59807d97e6d4',\n",
       " '0c0fbde5-b02c-459b-b984-a5ab34dbc5b2',\n",
       " '01fb6de0-1bd4-4f5e-90c2-be720257097e',\n",
       " '253829ed-79cf-41e1-bc68-858352e969c3',\n",
       " '186b8b06-062f-418e-bd3a-a477e84d7851',\n",
       " '313ab7b4-2660-4cfa-9f8c-c4a36a8ac69d',\n",
       " '914ed6c4-6928-4125-9192-b6698ee10632',\n",
       " '51625108-41ca-4344-a9d5-ec336c38c891',\n",
       " '238881f6-057c-4b5a-9567-a447e90a060d',\n",
       " 'a957d52f-608d-4454-a313-92536d356928',\n",
       " '20e2b946-d9f8-4d19-a89b-cf435e732fac',\n",
       " '6eea9d17-3cc3-4739-bb40-86f222650820',\n",
       " '6bf0dc6a-3123-4c01-8ab6-32083cb9109b',\n",
       " 'c0071269-741c-41d1-ac19-5d829cb5ba0b',\n",
       " '3bb739e3-8617-4b79-b8a9-b17f1e7a130d',\n",
       " 'e3cc6572-1a66-4884-bfae-741d2a0a6b60',\n",
       " '92d5e06f-4396-4410-8ccc-71251b2f707d',\n",
       " 'fd2b5656-29a8-418b-aa33-9cfe7907af6b',\n",
       " 'd686f297-8564-41cc-a386-2b9bc1b42db1',\n",
       " 'f3f5e845-c4c1-43ca-9da4-52f564e5eb10',\n",
       " 'ff2853ba-e83c-4e15-8f87-68773cf69072',\n",
       " '3b675c0c-d2ed-496d-9ff5-37f86d788dd4',\n",
       " '237b853f-b242-4a85-8a21-79ba22ea54e2',\n",
       " '9c17b00a-4b6e-499e-b53e-ac4225f3c3f7',\n",
       " '37938265-dac8-42f1-b435-1ea158c398f9',\n",
       " 'fd4905f5-2890-4d62-b2a8-afcff446ca32',\n",
       " 'dec36479-f99f-4458-8460-56f7e5ebc665',\n",
       " 'e696ac13-48e1-4aac-999d-445a7af4ee2f',\n",
       " '057c91b8-8818-44d8-9dbb-c10d0bd99495',\n",
       " 'ea30493b-2954-4ab2-826d-b5d28b4c3879',\n",
       " '40cf538f-8281-4740-a4fa-7723cedf5da3',\n",
       " '68fc2f5a-0364-48e1-9a3d-ef4d07b283e0',\n",
       " 'f20014ff-2ba5-4a85-9cce-f2d67ae6a1e6',\n",
       " '11caf3fb-5b86-42ea-bd39-dee4272fb095',\n",
       " '4971379b-f394-4d3a-a72c-e8564b6bed26',\n",
       " 'a56d7083-9e74-4fbf-8f34-790ebc34ec6d',\n",
       " '05d25f58-ec0e-459b-9e6c-161379194df7',\n",
       " 'ee88ec16-9e6d-4b78-a38b-deaf42ada954',\n",
       " 'ae946572-af43-4fb8-9812-e45ecd59a812',\n",
       " '67e95b42-6354-4eab-b852-4caeddb6b32f',\n",
       " '7f73f287-1caf-4b73-a82d-a020d5749ef7',\n",
       " '860aa147-29bb-4fb6-8a86-62de2cf0dd10',\n",
       " '6a269d62-6fa8-4279-bb4a-acd53673c742',\n",
       " '90ec4230-26d1-4ef7-9194-0ce5f7375592',\n",
       " 'f39a7735-0fc1-40c2-86e5-69f48ad4305a',\n",
       " '5bd387df-9a68-482a-a746-8f0635d8208c',\n",
       " '30d5f6c2-68e2-4ed9-8a30-915c3f08dc07',\n",
       " '4b8b8e39-2b4a-4855-b170-cf4990dd7aff',\n",
       " 'eb626a3c-9477-4927-abe2-21bc0a5d3a30',\n",
       " 'feb541a0-70ff-4a39-b70c-4673a80ec9e6',\n",
       " '3522c71a-845c-45f8-94e9-99abfc4a0c6f',\n",
       " '6826309b-e1c1-4326-b9c5-4168d2da34b6',\n",
       " '51f6f6af-b26d-49ab-9e83-cd0d3a99cda2',\n",
       " 'e34eeeaa-c393-45c5-987f-4e699ebc1cab',\n",
       " '9f66fd3e-6b33-4dea-b37f-a3873ba75815',\n",
       " 'a9bce285-f6c9-479b-b0a7-2df17a9d00aa',\n",
       " '5b6a07f6-57e6-4751-a554-dc6dd79ceba4',\n",
       " 'c7ff2773-74fc-4ca3-93a9-c5661dcf6d0c',\n",
       " '5219e835-150d-473f-a1d2-915210b04868',\n",
       " '1ea96b1d-9bab-4947-8c95-7d5e8e426dfc',\n",
       " '724df521-efb8-46af-88c1-09ded37d572c',\n",
       " 'aafb0719-35ed-4f06-a668-c42abc950fef',\n",
       " '0f4f9f22-a259-4b1b-88a4-14971eac90a2',\n",
       " '6a7f284e-e990-43aa-93da-513fe697dee1',\n",
       " '501918a9-f3e0-4c1b-83e9-6b4e9f4d4c4e',\n",
       " 'bc78e466-5633-48c1-a5cc-43a5ae88adc6',\n",
       " '3409e311-b5de-4b90-aa0b-a23353516b0c',\n",
       " '56b209af-4bed-4fcb-a9cd-3fc5bfe09fe6']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put into a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "\n",
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=VectorStoreQueryMode.DEFAULT, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug this into our RetrieverQueryEngine to synthesize a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_str = \"Create an iptables config for a web server 127.1.1.1 that only allows incoming traffic on port 80 and 443.\"\n",
    "# query_str = \"Generate Soot (verification) command that prints CFGs for all functions in java.util.concurrent.ThreadPoolExecutor class.\"\n",
    "# query_str = \"Generate an iptables config for an ecommerce website server that only allows incoming HTTP/HTTPS traffic, DNS lookup and SSH connecton.\"\n",
    "query_str = \"Generate Soot (verification) driver code that prints CFGs for all functions in `java.util.concurrent.ThreadPoolExecutor`\"\n",
    "\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "import java.util.concurrent.ThreadPoolExecutor;\n",
      "import soot.*;\n",
      "import soot.jimple.*;\n",
      "import soot.toolkits.graph.*;\n",
      "\n",
      "public class SootDriver {\n",
      "    public static void main(String[] args) {\n",
      "        // Initialize Soot\n",
      "        G.v().out.println(\"Initializing Soot...\");\n",
      "        Scene scene = new Scene();\n",
      "        scene.addBasicClass(\"java.util.concurrent.ThreadPoolExecutor\", SootClass.BODIES);\n",
      "\n",
      "        // Get the ThreadPoolExecutor class\n",
      "        SootClass threadPoolExecutorClass = scene.getSootClass(\"java.util.concurrent.ThreadPoolExecutor\");\n",
      "\n",
      "        // Iterate over all methods in the class\n",
      "        for (SootMethod method : threadPoolExecutorClass.getMethods()) {\n",
      "            // Skip non-static methods\n",
      "            if (!method.isStatic()) continue;\n",
      "\n",
      "            // Get the body of the method\n",
      "            SootBody body = method.retrieveActiveBody();\n",
      "\n",
      "            // Create a new CFG for the method\n",
      "            ControlFlowGraph cfg = new ControlFlowGraph(body);\n",
      "\n",
      "            // Print the CFG to standard output\n",
      "            G.v().out.println(\"CFG for \" + method.getName() + \":\");\n",
      "            G.v().out.println(cfg);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Docker\n",
      "\n",
      "- Use the Docker image from docker hub (`docker run -it noidsirius/soot_tutorial:latest`) or\n",
      "- Build the image locally (`docker build . -t soot_tutorial`) and run it (`docker run -it soot_tutorial`)\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "configcraft-ai-Ro5CM1mT-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
